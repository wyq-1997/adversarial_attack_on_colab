{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Zx52SlV8RwK7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","import shutil\n","drive.mount('/content/drive/')\n","load_path = '/content/drive/My Drive/yuqing/'\n","save_path = '.'\n","extract_path = {\n","    'ILSVRC2012_img_val.tar': 'imgs',\n","    'ILSVRC2012_devkit_t12.tar.gz': '.'\n","}\n","files = ('ILSVRC2012_devkit_t12.tar.gz', 'ILSVRC2012_img_val.tar', 'labels.txt')\n","for file in files:\n","    shutil.copy2(load_path+file, save_path)\n","    if 'tar' in file:\n","        shutil.unpack_archive(file, extract_path[file])\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":346},"executionInfo":{"elapsed":5048,"status":"error","timestamp":1616323007099,"user":{"displayName":"Jingcheng Wu","photoUrl":"","userId":"08373066062414867035"},"user_tz":-480},"id":"uB7Vg9eTKcz-","outputId":"37cc2020-1a17-48aa-bc34-66a1759d4e33"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-1-39f1d46e752a\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 60\u001b[0;31m \u001b[0mrlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadable_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-1-39f1d46e752a\u003e\u001b[0m in \u001b[0;36mget_rlabel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_rlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 51\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'labels.txt'"]}],"source":["\"\"\"\n","This script will demonstrate how to use a pretrained model, in PyTorch, \n","to make predictions. Specifically, we will be using VGG16 with a cat \n","image.\n","References used to make this script:\n","PyTorch pretrained models doc:\n","    http://pytorch.org/docs/master/torchvision/models.html\n","PyTorch image transforms example:\n","    http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms\n","Example code:\n","    http://blog.outcome.io/pytorch-quick-start-classifying-an-image/    \n","\"\"\"\n","\n","import io\n","import os\n","\n","from PIL import Image\n","import requests\n","import numpy as np\n","from tqdm import tqdm\n","\n","from torch.autograd import Variable\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import torch.nn\n","torch.manual_seed(1000)\n","\n","# Random cat img taken from Google\n","IMG_URL = 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg'\n","# Class labels used when training VGG as json, courtesy of the 'Example code' link above.\n","LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n","\n","# Let's get our class labels.\n","'''\n","Following two methods are for the readable prediction labels\n","'''\n","def get_rpred():\n","    response = requests.get(LABELS_URL)  # Make an HTTP GET request and store the response.\n","    labels = {int(key): value for key, value in response.json().items()}\n","    return labels\n","rpred = get_rpred()\n","\n","def readable_pred(pred):\n","    return rpred[pred]\n","\n","'''\n","Following two methods are for the readable target labels\n","'''\n","def get_rlabel():\n","    labels = {}\n","    for i, line in enumerate(open('labels.txt')):\n","        if i==0:\n","            continue\n","        elif i\u003e1000:\n","            break\n","        words = line.split()\n","        cat = ' '.join(words[1:])\n","        labels[i] = cat\n","    return labels\n","rlabel = get_rlabel()\n","\n","def readable_label(label):\n","    return rlabel[label]\n","\n","\n","# Now that we have an img, we need to preprocess it.\n","# We need to:\n","#       * resize the img, it is pretty big (~1200x1200px).\n","#       * normalize it, as noted in the PyTorch pretrained models doc,\n","#         with, mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n","#       * convert it to a PyTorch Tensor.\n","#\n","# We can do all this preprocessing using a transform pipeline.\n","min_img_size = 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n","transform_pipeline = transforms.Compose([transforms.Resize(min_img_size),\n","                    transforms.ToTensor(),\n","                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                std=[0.229, 0.224, 0.225])])\n","def preprocess(img):\n","    if img.mode=='L':\n","        img = img.convert(\"RGB\")\n","    img = transform_pipeline(img)\n","    # PyTorch pretrained models expect the Tensor dims to be (num input imgs, num color channels, height, width).\n","    # Currently however, we have (num color channels, height, width); let's fix this by inserting a new axis.\n","    img = img.unsqueeze(0)  # Insert the new axis at index 0 i.e. in front of the other axes/dims. \n","    # Now that we have preprocessed our img, we need to convert it into a \n","    # Variable; PyTorch models expect inputs to be Variables. A PyTorch Variable is a  \n","    # wrapper around a PyTorch Tensor.\n","    img = Variable(img)\n","    return img\n","\n","'''\n","get prediction given an image and the neural network\n","'''\n","def get_pred(nn, img):\n","    prediction = nn(img)  # Returns a Tensor of shape (batch, num class labels)\n","    prediction = prediction.data.numpy().argmax()  # Our prediction will be the index of the class label with the largest value.\n","    return prediction\n","\n","'''\n","following two methods are to convert prediction label distribution to the target\n","label distribution\n","'''\n","def get_mapping():\n","    mapping = {}\n","    for pred_key, pred_val in rpred.items():\n","        for label_key, label_val in rlabel.items():\n","            if label_val==pred_val:\n","                mapping[pred_key] = label_key\n","                break\n","        if pred_key not in mapping:\n","            print(mapping)\n","            print(pred_val)\n","            assert pred_key in mapping\n","    return mapping\n","mapping = get_mapping()\n","\n","def map_pred_to_label(pred):\n","    return mapping[pred]\n","\n","'''\n","the method to get adversarial attack\n","'''\n","noise_scale=0.02\n","def get_adversarial_example(nn, img, output, target):\n","    criterion = torch.nn.CrossEntropyLoss()\n","    target = torch.LongTensor([target])\n","    loss = criterion(output, target)\n","    # retain_graph=True，calculate all grads of vars with requires_grad=True\n","    loss.backward(retain_graph=True)\n","    img_grad = torch.sign(img.grad.data)\n","    return img+noise_scale*img_grad\n","\n","'''\n","The main loop to pred and attack\n","'''\n","def main_loop(nn):\n","    label_file = open('/content/ILSVRC2012_devkit_t12/data/ILSVRC2012_validation_ground_truth.txt')\n","    cnt, acc, fooled = 0, 0, 0\n","    size = len(os.listdir('imgs'))\n","    sample_gap = size//1000\n","    for label, img_file in zip(label_file, sorted(os.listdir('imgs'))):\n","        label = int(label)\n","        img = Image.open(f\"imgs/{img_file}\")\n","        # img = preprocess(img)\n","        if img.mode=='L':\n","            img = img.convert(\"RGB\")\n","        img = transform_pipeline(img)\n","        img = img.unsqueeze(0)\n","        img = Variable(img, requires_grad=True)\n","        \n","        output = nn(img)\n","        pred = output.data.numpy().argmax()\n","        # pred_top2 = pred[1]\n","        # print(pred, type(pred), type(pred[0]))\n","\n","        # label = label-1 #make it from 1~1000 to 0~999\n","\n","        mapped_pred = map_pred_to_label(pred)\n","        cnt += 1\n","        if(map_pred_to_label(pred)==label):\n","            acc+=1\n","            attacked_img = get_adversarial_example(nn, img, output, pred)\n","            attacked_pred = get_pred(nn, attacked_img)\n","            if(map_pred_to_label(attacked_pred)!=label):\n","                fooled+=1\n","        if(cnt%sample_gap==0):\n","            print(f\"{fooled}/{acc}/{cnt}/{size}, acc:{acc/cnt:.2f}, fooling rate:{0.0 if acc==0 else fooled/acc:.2f}\")\n","            # print(pred, label)\n","            # print(img_file, pred, readable_pred(pred), label, readable_label(label))\n","    label_file.close()\n","\n","# Let's get the cat img.\n","# response = requests.get(IMG_URL)\n","# img = Image.open(io.BytesIO(response.content))  # Read bytes and store as an img.\n","# print(img.size, \"stop!\")\n","# exit(0)\n","# Now let's load our model and get a prediciton!\n","vgg = models.vgg16(pretrained=True)  # This may take a few minutes.\n","main_loop(vgg)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1871970,"status":"ok","timestamp":1615902725340,"user":{"displayName":"Jingcheng Wu","photoUrl":"","userId":"08373066062414867035"},"user_tz":-480},"id":"-9OGvLChvLZu","outputId":"225e4c04-1d55-4a63-99ce-e113efea0a4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2021-03-16 13:20:54--  http://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n","Resolving image-net.org (image-net.org)... 171.64.68.16\n","Connecting to image-net.org (image-net.org)|171.64.68.16|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6744924160 (6.3G) [application/x-tar]\n","Saving to: ‘ILSVRC2012_img_val.tar’\n","\n","ILSVRC2012_img_val. 100%[===================\u003e]   6.28G  4.13MB/s    in 31m 9s  \n","\n","2021-03-16 13:52:03 (3.44 MB/s) - ‘ILSVRC2012_img_val.tar’ saved [6744924160/6744924160]\n","\n","--2021-03-16 13:52:03--  http://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n","Resolving image-net.org (image-net.org)... 171.64.68.16\n","Connecting to image-net.org (image-net.org)|171.64.68.16|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2568145 (2.4M) [application/x-gzip]\n","Saving to: ‘ILSVRC2012_devkit_t12.tar.gz’\n","\n","ILSVRC2012_devkit_t 100%[===================\u003e]   2.45M  2.43MB/s    in 1.0s    \n","\n","2021-03-16 13:52:04 (2.43 MB/s) - ‘ILSVRC2012_devkit_t12.tar.gz’ saved [2568145/2568145]\n","\n"]}],"source":["!wget http://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n","!wget http://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":170205,"status":"ok","timestamp":1615911487777,"user":{"displayName":"Jingcheng Wu","photoUrl":"","userId":"08373066062414867035"},"user_tz":-480},"id":"e9BIFbHqP0bP","outputId":"27178405-387d-48ba-c716-b0e2e794b588"},"outputs":[{"name":"stdout","output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com\u0026redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob\u0026scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native\u0026response_type=code\n","\n","Enter your authorization code:\n","4/1AY0e-g6QUgdcVft15lwT2uox36NFvsvOBOpYbnBbvbmCrTzj5IXZa-0thIM\n","Mounted at /content/drive\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/yuqing/ILSVRC2012_devkit_t12.tar.gz'"]},"execution_count":2,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from google.colab import drive\n","import shutil\n","drive.mount('/content/drive')\n","save_path = '/content/drive/My Drive/yuqing'\n","shutil.copy2('ILSVRC2012_img_val.tar', save_path)\n","shutil.copy2('ILSVRC2012_devkit_t12.tar.gz', save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2400,"status":"ok","timestamp":1615911927637,"user":{"displayName":"Jingcheng Wu","photoUrl":"","userId":"08373066062414867035"},"user_tz":-480},"id":"XhGKrPxo1jze","outputId":"06a83e30-e863-471d-e80e-a20df116caf5"},"outputs":[{"name":"stdout","output_type":"stream","text":["ls: cannot access 'ILSVRC2012_devkit_t12.tar.gz/data': Not a directory\n"]}],"source":["!ls ILSVRC2012_devkit_t12.tar.gz/data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":834,"status":"ok","timestamp":1615392419662,"user":{"displayName":"Jingcheng Wu","photoUrl":"","userId":"08373066062414867035"},"user_tz":-480},"id":"zA5WN8v0qNdP","outputId":"07b688e3-ec5e-41c6-c994-d4bfe818778c"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['info', 'images', 'licenses', 'categories'])\n","{'license': 2, 'file_name': 'COCO_test2014_000000523573.jpg', 'coco_url': 'http://images.cocodataset.org/test2014/COCO_test2014_000000523573.jpg', 'height': 500, 'width': 423, 'date_captured': '2013-11-14 12:21:59', 'id': 523573}\n","{'supercategory': 'person', 'id': 1, 'name': 'person'}\n"]}],"source":["import torchvision.models as models\n","import torchvision\n","import json\n","\n","with open('annotations/image_info_test2014.json') as f:\n","  data = json.load(f)\n","print(data.keys())\n","print(data['images'][0])\n","print(data['categories'][0])\n","# for image, c in zip(data['images'], data['categories']):\n","#     print(image['file_name'], c)\n","# exit(0)\n","# vgg = models.vgg16(pretrained=True)\n","# imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\n","# data_loader = torch.utils.data.DataLoader(imagenet_data,\n","#                     batch_size=4,\n","#                     shuffle=True,\n","#                     num_workers=args.nThreads)\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"AA.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}